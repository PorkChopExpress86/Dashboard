#!/usr/bin/env python3"""Selenium-based scraper for Schoolcafe menus.Usage (PowerShell, local host):  python scripts\scrape_schoolcafe.py --school "Post Elementary" --grade 1Usage (Docker scraper service):  docker compose run --rm scraperEnv:  SELENIUM_REMOTE_URL (optional) â€” if set, use remote Selenium server, e.g. http://selenium:4444"""from __future__ import annotationsimport argparseimport jsonimport osimport timeimport refrom datetime import datetime, date, timedeltafrom bs4 import BeautifulSoupfrom selenium import webdriverfrom selenium.common.exceptions import TimeoutException, NoSuchElementExceptionfrom selenium.webdriver.common.by import Byfrom selenium.webdriver.common.keys import Keysfrom selenium.webdriver.chrome.options import Optionsfrom selenium.webdriver.support.ui import WebDriverWaitfrom selenium.webdriver.support import expected_conditions as ECfrom webdriver_manager.chrome import ChromeDriverManagerBASE_URL = "https://www.schoolcafe.com/menus"CFISD_URL = "https://www.schoolcafe.com/CFISD"# Direct menu URL with viewID for Post Elementary, Grade 1, LunchCFISD_MENU_URL = "https://www.schoolcafe.com/CFISD/menus?viewID=4322524b-1f7e-476a-9139-814c671143ef"def create_driver(headless: bool = True) -> webdriver.Chrome:    options = Options()    if headless:        options.add_argument("--headless=new")    options.add_argument("--no-sandbox")    options.add_argument("--disable-dev-shm-usage")    options.add_argument("--disable-gpu")    options.add_argument("--window-size=1920,1200")    remote_url = os.environ.get("SELENIUM_REMOTE_URL")    if remote_url:        return webdriver.Remote(command_executor=remote_url, options=options)    from selenium.webdriver.chrome.service import Service    service = Service(ChromeDriverManager().install())    driver = webdriver.Chrome(service=service, options=options)    return driverdef find_search_input(driver: webdriver.Chrome, timeout: int = 12):    wait = WebDriverWait(driver, timeout)    selectors = [        (By.CSS_SELECTOR, "input[type='search']"),        (By.CSS_SELECTOR, "input[placeholder*='Search']"),        (By.CSS_SELECTOR, "input[aria-label*='search']"),        (By.CSS_SELECTOR, "input[type='text']"),        (By.XPATH, "//input"),    ]    for by, sel in selectors:        try:            el = wait.until(EC.presence_of_element_located((by, sel)))            if el.is_displayed():                return el        except TimeoutException:            continue    raise NoSuchElementException("Search input not found")def click_school_link(driver: webdriver.Chrome, school_name: str, timeout: int = 12) -> bool:    wait = WebDriverWait(driver, timeout)    xpath_candidates = [        f"//a[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), '{school_name.lower()}')]",        f"//div[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), '{school_name.lower()}')]/ancestor::a[1]",        f"//button[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), '{school_name.lower()}')]",    ]    for xp in xpath_candidates:        try:            el = wait.until(EC.element_to_be_clickable((By.XPATH, xp)))            driver.execute_script("arguments[0].scrollIntoView(true);", el)            time.sleep(0.2)            el.click()            return True        except TimeoutException:            continue        except Exception:            continue    return Falsedef try_select_grade(driver: webdriver.Chrome, grade: str, timeout: int = 8) -> bool:    wait = WebDriverWait(driver, timeout)    try:        select_el = driver.find_element(By.TAG_NAME, "select")        options = select_el.find_elements(By.TAG_NAME, "option")        for opt in options:            if grade in opt.text:                opt.click()                return True    except Exception:        pass    xpath = f"//button[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), '{grade.lower()}') or contains(., '{grade}')]"    try:        btn = wait.until(EC.element_to_be_clickable((By.XPATH, xpath)))        btn.click()        return True    except Exception:        pass    return Falsedef extract_menu_from_html(html: str) -> dict:    soup = BeautifulSoup(html, "lxml")    data = {"days": []}    table = soup.find("table")    if table:        headers = [th.get_text(strip=True) for th in table.find_all("th")]        rows = []        for tr in table.find_all("tr"):            cells = [td.get_text(" ", strip=True) for td in tr.find_all(["td", "th"]) if td.get_text(strip=True)]            if cells:                rows.append(cells)        data["table_headers"] = headers        data["table_rows"] = rows        return data    day_blocks = soup.find_all(class_=lambda n: n and ("day" in n or "menu" in n))    if day_blocks:        for block in day_blocks:            title = block.find(lambda tag: tag.name in ["h3", "h4", "h2"]) or block.find("strong")            items = [li.get_text(strip=True) for li in block.find_all("li")] or [s.get_text(strip=True) for s in block.find_all("span")]            if items:                data["days"].append({"title": title.get_text(strip=True) if title else "", "items": items})        if data["days"]:            return data    text = soup.get_text("\n")    lines = [l.strip() for l in text.splitlines() if l.strip()]    data["lines"] = lines[:200]    return datadef normalize_week(extracted: dict) -> dict:    """Attempt to convert extracted raw structure into a list of day objects for the current week.    Heuristics:    1. If table_rows exist and first row looks like headers containing day names or dates, align columns.    2. If extracted["days"] already present, try to map titles to actual dates of current week.    3. Fallback: scan lines for patterns like 'Mon', 'Tuesday', or date formats (MM/DD or Month DD).    """    week = []    today = date.today()    # Determine Monday of current week    monday = today - timedelta(days=today.weekday())    weekday_names = ["Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"]    short_weekday = {d[:3].lower(): d for d in weekday_names}    def date_for_name(name: str) -> date | None:        n = name.strip().lower()        for w in weekday_names:            if w.lower() in n:                # Map to current week                idx = weekday_names.index(w)                return monday + timedelta(days=idx)        # Look for mm/dd        m1 = re.search(r"(\d{1,2})/(\d{1,2})", n)        if m1:            try:                dt = datetime(year=today.year, month=int(m1.group(1)), day=int(m1.group(2))).date()                # Only accept if within +/- 7 days of current week                if monday - timedelta(days=3) <= dt <= monday + timedelta(days=10):                    return dt            except ValueError:                pass        # Month name pattern        m2 = re.search(r"(jan|feb|mar|apr|may|jun|jul|aug|sep|oct|nov|dec)[a-z]*\s+(\d{1,2})", n)        if m2:            try:                month_str = m2.group(1)                month_num = {                    'jan':1,'feb':2,'mar':3,'apr':4,'may':5,'jun':6,'jul':7,'aug':8,'sep':9,'oct':10,'nov':11,'dec':12                }[month_str]                dt = datetime(year=today.year, month=month_num, day=int(m2.group(2))).date()                if monday - timedelta(days=3) <= dt <= monday + timedelta(days=10):                    return dt            except ValueError:                pass        return None    # Strategy 1: table headers    headers = extracted.get("table_headers") or []    rows = extracted.get("table_rows") or []    if headers and rows:        # Build columns for each header -> list of items per row (excluding header row if duplicated)        # Assume first row might be headers again; skip if identical        header_dates = [date_for_name(h) for h in headers]        if any(header_dates):            # Transpose rows (excluding any header-like first row duplication)            data_rows = rows            if rows and all(cell in headers for cell in rows[0]):                data_rows = rows[1:]            # Collect items per column            columns = [[] for _ in headers]            for r in data_rows:                for ci, cell in enumerate(r):                    if ci < len(columns) and cell.strip():                        columns[ci].append(cell.strip())            for hi, h in enumerate(headers):                d = header_dates[hi] or date_for_name(h) or (monday + timedelta(days=hi) if hi < 7 else None)                if d:                    week.append({                        "date": d.isoformat(),                        "weekday": weekday_names[d.weekday()],                        "items": columns[hi]                    })            if week:                return {"week": week}    # Strategy 2: extracted["days"] with titles    for day in extracted.get("days", []):        title = day.get("title", "")        d = date_for_name(title)        if d:            week.append({                "date": d.isoformat(),                "weekday": weekday_names[d.weekday()],                "items": day.get("items", [])            })    if week:        return {"week": week}    # Strategy 3: scan lines sequentially for day markers    lines = extracted.get("lines", [])    current_day = None    day_map: dict[str, list[str]] = {}    for line in lines:        low = line.lower()        matched_day = None        for key in short_weekday:            if re.match(rf"^{key}[\s:,-]", low) or low.startswith(short_weekday[key].lower()):                matched_day = short_weekday[key]                break        if matched_day:            current_day = matched_day            day_map.setdefault(current_day, [])            continue        if current_day:            # Filter out overly generic lines            if len(line) > 2 and not re.match(r"^\W+$", line):                day_map[current_day].append(line)    if day_map:        for wd, items in day_map.items():            d_idx = weekday_names.index(wd)            d = monday + timedelta(days=d_idx)            week.append({"date": d.isoformat(), "weekday": wd, "items": items})        return {"week": week}    # Fallback: empty week    return {"week": []}def navigate_guest_flow(driver, school: str | None, grade: str | None):    """Simplified navigation for CFISD direct URL:    1. Start at https://www.schoolcafe.com/CFISD    2. Click 'View Menus as Guest' button or link    3. Select school (Post Elementary)    4. Select grade (1 or 01)    5. Select meal type (Lunch)    6. Wait for menu items to appear    """    wait = WebDriverWait(driver, 20)    print(f"[guest-flow] Current URL: {driver.current_url}")        # Step 1: Click "View Menus (as a guest)" link    try:        time.sleep(2)        guest_link_xpaths = [            "//a[contains(., 'View Menus (as a guest)')]",            "//a[contains(., 'View Menus')]",            "//button[contains(., 'View Menus (as a guest)')]",            "//button[contains(., 'View Menus')]",            "//*[contains(text(), 'View Menus (as a guest)')]",        ]        clicked = False        for xp in guest_link_xpaths:            try:                elem = wait.until(EC.element_to_be_clickable((By.XPATH, xp)))                print(f"[guest] Found element: {elem.text}")                driver.execute_script("arguments[0].scrollIntoView(true);", elem)                time.sleep(0.5)                driver.execute_script("arguments[0].click();", elem)                time.sleep(3)                clicked = True                print(f"[guest] Clicked: {elem.text}")                print(f"[guest] New URL: {driver.current_url}")                break            except Exception as ex:                continue        if not clicked:            print("[guest] Could not find 'View Menus (as a guest)' link")    except Exception as e:        print(f"[guest] Error: {e}")    # Step 2: Select school from dropdown or search    if school:        try:            time.sleep(3)                        # This is a Material UI site - look for the school selector by text            # First, find and click the "Select School" element to open dropdown            school_selectors = [                "//div[contains(text(), 'Select School')]",                "//*[contains(text(), 'Select School')]",                "//label[contains(text(), 'Select School')]/following-sibling::div",            ]                        clicked_dropdown = False            for xpath in school_selectors:                try:                    elem = wait.until(EC.element_to_be_clickable((By.XPATH, xpath)))                    print(f"[school] Found 'Select School' element")                    driver.execute_script("arguments[0].scrollIntoView(true);", elem)                    time.sleep(0.5)                    driver.execute_script("arguments[0].click();", elem)                    time.sleep(2)                    clicked_dropdown = True                    print(f"[school] Clicked 'Select School' dropdown")                    break                except Exception as ex:                    continue                        if not clicked_dropdown:                print(f"[school] Could not find 'Select School' dropdown")                matched = False            else:                # Now find and click the school option from the opened dropdown                time.sleep(2)  # Wait longer for options to load                matched = False                                # Debug: print all available options                all_options = driver.find_elements(By.XPATH, "//li | //div[@role='option']")                print(f"[school] Found {len(all_options)} options in DOM")                for i, opt in enumerate(all_options[:20]):                    print(f"[school]   Option {i}: {opt.text[:50]}")                                option_selectors = [                    f"//li[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), '{school.lower()}')]",                    f"//div[@role='option' and contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), '{school.lower()}')]",                    f"//*[@role='option' and contains(text(), '{school}')]",                ]                                for xpath in option_selectors:                    try:                        option = wait.until(EC.element_to_be_clickable((By.XPATH, xpath)))                        print(f"[school] Found option: {option.text}")                        driver.execute_script("arguments[0].click();", option)                        time.sleep(2)                        matched = True                        print(f"[school] Selected: {option.text}")                        break                    except Exception as ex:                        continue                                if not matched:                    print(f"[school] Could not select '{school}' from dropdown options")        except Exception as e:            print(f"[school] Error: {e}")    # Step 3: Select grade    if grade:        try:            time.sleep(2)                        # Click the "Select Grade" dropdown            grade_selectors = [                "//div[contains(text(), 'Select Grade')]",                "//*[contains(text(), 'Select Grade')]",            ]                        clicked_dropdown = False            for xpath in grade_selectors:                try:                    elem = wait.until(EC.element_to_be_clickable((By.XPATH, xpath)))                    print(f"[grade] Found 'Select Grade' element")                    driver.execute_script("arguments[0].scrollIntoView(true);", elem)                    time.sleep(0.5)                    driver.execute_script("arguments[0].click();", elem)                    time.sleep(2)                    clicked_dropdown = True                    print(f"[grade] Clicked 'Select Grade' dropdown")                    break                except Exception as ex:                    continue                        if not clicked_dropdown:                print(f"[grade] Could not find 'Select Grade' dropdown")            else:                # Now find and click the grade option                time.sleep(1.5)                matched = False                # Look for grade patterns: "01", "1", "1st", "Grade 1", etc.                grade_patterns = [                    f"//li[contains(text(), '{grade}') or contains(text(), '0{grade}') or contains(text(), '{grade}st') or contains(text(), 'Grade {grade}')]",                    f"//div[@role='option' and (contains(text(), '{grade}') or contains(text(), '0{grade}'))]",                    f"//*[@role='option' and contains(text(), '{grade}')]",                ]                                for xpath in grade_patterns:                    try:                        option = wait.until(EC.element_to_be_clickable((By.XPATH, xpath)))                        print(f"[grade] Found option: {option.text}")                        driver.execute_script("arguments[0].click();", option)                        time.sleep(2)                        matched = True                        print(f"[grade] Selected: {option.text}")                        break                    except Exception as ex:                        continue                                if not matched:                    print(f"[grade] Could not select grade '{grade}' from dropdown options")        except Exception as e:            print(f"[grade] Error: {e}")    # Step 4: Select meal type (Lunch)    try:        time.sleep(2)                # Click the "Select Meal Type" dropdown        meal_selectors = [            "//div[contains(text(), 'Select Meal Type')]",            "//*[contains(text(), 'Select Meal Type')]",        ]                clicked_dropdown = False        for xpath in meal_selectors:            try:                elem = wait.until(EC.element_to_be_clickable((By.XPATH, xpath)))                print(f"[meal] Found 'Select Meal Type' element")                driver.execute_script("arguments[0].scrollIntoView(true);", elem)                time.sleep(0.5)                driver.execute_script("arguments[0].click();", elem)                time.sleep(2)                clicked_dropdown = True                print(f"[meal] Clicked 'Select Meal Type' dropdown")                break            except Exception as ex:                continue                if not clicked_dropdown:            print(f"[meal] Could not find 'Select Meal Type' dropdown")            matched = False        else:            # Now find and click the Lunch option            time.sleep(1.5)            matched = False            lunch_selectors = [                "//li[contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'lunch')]",                "//div[@role='option' and contains(translate(., 'ABCDEFGHIJKLMNOPQRSTUVWXYZ', 'abcdefghijklmnopqrstuvwxyz'), 'lunch')]",                "//*[@role='option' and contains(text(), 'Lunch')]",            ]                        for xpath in lunch_selectors:                try:                    option = wait.until(EC.element_to_be_clickable((By.XPATH, xpath)))                    print(f"[meal] Found option: {option.text}")                    driver.execute_script("arguments[0].click();", option)                    time.sleep(2)                    matched = True                    print(f"[meal] Selected: {option.text}")                    break                except Exception:                    continue                if not matched:            print("[meal] Could not select 'Lunch'")    except Exception as e:        print(f"[meal] Error: {e}")    # Step 9: Wait for menu items to appear    try:        time.sleep(3)  # Give time for menu to load        print("[menu] Waiting for menu items to appear...")    except Exception as e:        print(f"[menu] Wait error: {e}")def scrape_school_menu(school_name: str, grade: str, headless: bool = True, state: str | None = None, district: str | None = None, view_id: str | None = None) -> dict:    driver = create_driver(headless=headless)    try:        # Use direct menu URL with viewID - bypasses all navigation!        menu_url = f"https://www.schoolcafe.com/CFISD/menus?viewID={view_id}" if view_id else CFISD_MENU_URL                print(f"[info] Loading menu URL: {menu_url}")        driver.get(menu_url)        time.sleep(5.0)  # Wait for menu to load                print(f"[debug] Current URL: {driver.current_url}")        print(f"[debug] Page title: {driver.title}")                # Save final page HTML        try:            with open("/work/cache/menu_page.html", "w", encoding="utf-8") as f:                f.write(driver.page_source)            print("[debug] Saved menu_page.html to cache/")        except Exception as e:            print(f"[debug] Could not save menu HTML: {e}")                # Extract menu items        html = driver.page_source        soup = BeautifulSoup(html, "lxml")                menu_items = []                # Look for menu item titles        item_selectors = [            "div.menu-item",            "div.menuitem",            "div[class*='menu-item']",            "div[class*='MenuItem']",            "div[class*='food']",            "li[class*='menu']",            "span[class*='menu-item']",        ]                for selector in item_selectors:            items = soup.select(selector)            if items:                for item in items:                    text = item.get_text(strip=True)                    if text and 3 < len(text) < 150:                        menu_items.append(text)                        print(f"[menu-item] Found: {text}")                result = extract_menu_from_html(html)        result.update({            "school": school_name,            "grade_requested": grade,            "url": driver.current_url,            "menu_items_found": list(set(menu_items))[:50]        })        return result    finally:        driver.quit()def main() -> None:    parser = argparse.ArgumentParser(description="Scrape Schoolcafe menu for a school and grade")    parser.add_argument("--school", required=True, help="School name (e.g. 'Post Elementary')")    parser.add_argument("--grade", default="1", help="Grade to filter (e.g. '1')")    parser.add_argument("--no-headless", dest="headless", action="store_false", help="Run browser visible for debugging")    parser.add_argument("--state", help="State name for guest flow (e.g. 'Texas')")    parser.add_argument("--district", help="District name for guest flow")`nparser.add_argument("--view-id", dest="view_id", default=None, help="Direct viewID from menu URL")`nargs = parser.parse_args()    data = scrape_school_menu(args.school, args.grade, headless=args.headless, state=args.state, district=args.district, view_id=args.view_id)    normalized = normalize_week(data)    # Merge, normalized week takes precedence    output = {**data, **normalized}    print(json.dumps(output, indent=2, ensure_ascii=False))if __name__ == "__main__":    main()